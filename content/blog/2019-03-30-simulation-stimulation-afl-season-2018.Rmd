---
title: 'Simulation Stimulation: AFL Season 2018'
author: Joshua White
date: '2019-03-30'
slug: simulation-stimulation-afl-season-2018
categories:
  - R
tags:
  - AFL
  - Sport   
  - Simulations
image:
  caption: ''
  focal_point: ''
output:
  blogdown::html_page:
    toc: true
summary: "Attempts at modelling and simulating the 2018 Australian Football League Season"
header:
  image: "headers/GeelongCats.jpg"
  caption: "image source: [**The Courier**](www.thecourier.com.au)"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source("blogfiles/AFL-Sim/AFLsim2018.R")
```


Recently I was, as I am (unfortunately) rather wont to do, aimlessly watching YouTube videos when I came across these [two](https://www.youtube.com/watch?v=Vv9wpQIGZDw) [videos](https://www.youtube.com/watch?v=Zs2M7gWSbTg) by Numberphile. Here, the authors discuss their attempts at modelling and simulating seasons of the English Premier League. 

As an avid [AFL](https://en.wikipedia.org/wiki/Australian_Football_League) fan, and with the upcoming season fast approaching (edit, now well underway, [carn](https://www.urbandictionary.com/define.php?term=carn) [the cats!](https://www.afl.com.au/news/2019-03-22/match-report-collingwood-v-geelong)), why not try to apply these methods to the AFL season.  This post will be the first of a three part series doing so. In part 1, I will discuss the models and compare their results to the actual data from the 2018 season. Part 2 will apply the models to the 2019 season. And part 3 will go through the implementation details and show my code.  

## The Models

###Poisson Distributed Team Scores
Okay, lets discuss the first model. Basically, each team's score in every game is modeled as a random drawing from a Poisson distribution --- i.e. $P(k) = e^{-\lambda} \frac{\lambda^k}{k!}$, where k is the number of points scored. A Poisson distribution is great for fast and simple modelling because it requires the estimation of only one parameter, $\lambda$. On the other hand, it isn't exactly suited to the AFL context because the model assumes that all 'events' (in this case, scoring a point) occur independently. In AFL, a goal is worth 6 points, violating this assumption.  

In any case, let's press on with that in mind (we are not looking for a perfect model here). We calculate $\lambda$ for each game, based on the points scored for and against, at home and away, for each team in 2018:

$\lambda_{home team} =$ League Average points scored at home $\times$ home team attack strength at home $\times$ away team defense strength away

$\lambda_{away team} =$ League Average points scored away $\times$ away team attack strength away $\times$ hometeam defense strength at home

As you can see, the model takes into account the general home advantage as well as each particular team's attack and defensive strengths at home and away. The strength parameters refereed to above are simply the averages for that particular team, relative to the league-wide average. 

###Normally Distributed team scores
This model is a simple extension to the Poisson-based model above. However, it should be superior to it because: 

1. There is no assumption of independence, and 
2. It has an extra parameter to capture differences among teams in the variance of their defense and attack strength. 

Here, it is assumed that team scores are normally distributed around $\lambda$ as defined above, with a standard deviation calculated as the pooled standard deviation of that teams 'for' scores standard deviation, and the opponents 'against' scores standard deviation:
$$Points_{teamA} \sim \mathcal{N}(\mu=\lambda,\,\sigma = \sigma_p)\,$$ 
where: <br>
$$\sigma_{p}=\frac{\sigma_{teamA\>for} + \sigma_{teamB\>against}}{2}$$

###Rankings model: tanh
This model simply relies on the ordinal data of last years final Ladder: Carlton, the wooden spoon winners (i.e. the last placeholders, out of 18 teams) get a ranking of 1, and Richmond, who finished on top of the table get a ranking of 18. Then, with this information, the probability of team A winning is modelled as follows:

$$P(A\>wins)=\frac{tanh(\frac{a-b}{w} + d)+1}{2}$$
where:<br>
A = home team <br>
a = home team ranking <br> 
b = away team ranking <br>
w = weighting parameter (the higher the number the less weight placed on the rankings) <br>
d = draw paramater, to allow for possibility of draws, calculated as so that $tanh(d) = -draw\>rate$

The draw rate, $\frac{7}{990} \approx 0.007$, was based on the likelihood of drawing a game over the past 5 years.

The probability of B winning is calculated in the same manner as above, except with $b-a$. 

Finally, a draw is the remaining probability: $P(Draw)= 1 - P(A\>wins) - P(B\>wins)$. This means that the draw rate will be greater for greater rankings differentials, and greater for greater weight parameters. 

## The Data

```{r data}
pointstidy
fixture2019
```


##Results

Ok, so we are going to simulate 1000 seasons for each of these models (I wish I could do more, but my current computer lacks power, and my code is far from effiecient :cry: ). Here are the results from these simulations (for details on the code see part III of this series):

###Poisson model
```{r poisresults}
Finalsimtablepois.2018

```

###Normal model
```{r normresults}
Finalsimtablenorm.2018

```

###Tanh model

Here, we will inspect our restuls for our model wih various different weighting paramaters, w, before taking the best of these for use in our further analyses. 

```{r tanhresults}
Finalsimlist.2018[[1]] %>% tibble()
Finalsimlist.2018[[20]] %>% tibble()
Finalsimlist.2018[[50]] %>% tibble()
Finalsimlist.2018[[100]] %>% tibble()
Finalsimlist.2018[[500]] %>% tibble()
Finalsimlist.2018[[1000]] %>% tibble()

```

We are going to have to play it by ear a little, and use our intiution to see which of these models appears to be the best for us going forward.  Particularly, because we have used the same data that trained the model to test the model, and we want to avoid overfitting. So, we will pick the best model based on (1) it's spearman coefficient with last year's ladder and (2) its ability to have varation in our simulations (the same team doesn't simply win each year). 

```{r tanhcomparisons}
##Create order comparisons table##  
for (w in c(1, 20, 50, 100, 500, 1000)){
  name <- paste0("Ordertanh.", w)
  assign(name,  match(finalTable2018$Team, Finalsimlist.2018[[w]]$Team))
  
}

ordercomparison.2018.tanh <- data.frame(Team = factor(finalTable2018$Team),
                                        Order2018 = 1:18,
                                        Ordertanh.1,
                                        Ordertanh.20,
                                        Ordertanh.50,
                                        Ordertanh.100, 
                                        Ordertanh.500,
                                        Ordertanh.1000)
ordercomparison.2018.tanh

## get correlation matrix##
cormatrix.weights <- cor(ordercomparison.2018.tanh[,2:ncol(ordercomparison.2018.tanh)], use = "all.obs", method = "spearman")

# lets find a nice way to graphically look at that
ggcorrplot(cormatrix.weights, lab = TRUE, show.legend = TRUE)
```

So on this basis, we will choose a weight parameter value of 20 as, on inspection of simulation results, it allows for enough randomness in the system to allow teams in the top 8 to have some reasonable likelihood of winning the cup, but (all but) denies the chance for the really useless teams for winning. Let's be honest, carlton's chance of winning the premiership is less than 1 in a million. 

##Comparing the three models.

```{r comparing the models}

#create table with different orders. 
ordercomparison.2018 <- data.frame(Team = factor(finalTable2018$Team),
                              Order2018 = 1:18,
                              OrderNorm.2018 = match(finalTable2018$Team, Finalsimtablenorm.2018$Team),
                              OrderPois.2018 = match(finalTable2018$Team, Finalsimtablepois.2018$Team),
                              OrderTanh.2018 = match(finalTable2018$Team, Finalsimlist.2018[[20]]$Team)) 

ordercomparison.2018 %>% tibble()

#Let's now get the spearman's correlation coefficients and show that in a correlatin matrix
cormatrix.2018 <- cor(ordercomparison.2018[,2:5], use = "all.obs", method = "spearman")
ggcorrplot(cormatrix.2018, lab = TRUE)
```

##Graphs

SO now lets have a look at this data graphically. 

### Poisson Results Graph

```{r poisgraph}
#graphing and comparing the two models. 

#create mode function
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
###GRAPH FOR POISSON MODEL###

#transpose data to tidy form for use in ggplot
SimResultspois.2018$simNumber <- sprintf("sim%d", seq(1:Nsim.pois.2018))
SimResultspois.2018.tidy <- gather(SimResultspois.2018,"Team", "Position", -simNumber)


#create ordered factor to order graph
SimResultspois.2018.tidy$Team <- factor(SimResultspois.2018.tidy$Team, levels = Finalsimtablepois.2018$Team)

#create graph 
ggplot(SimResultspois.2018.tidy, aes(x = Team, y = Position)) + 
  geom_violin(aes(fill = Team, col = Team), show.legend = FALSE,
              trim = TRUE, kernel = "gaussian", adjust = 3) + 
  stat_summary(fun.y=median, geom="point",color="black", 
                     aes(shape = "Median", size="Median")) +
  stat_summary(fun.y=mean, geom ="point", color = "black",
                     aes(shape = "Mean", size = "Mean")) +
  stat_summary(fun.y=Mode, geom ="point", color = "black",
                     aes(shape = "Mode", size = "Mode")) +
  coord_flip(ylim = c(1,18)) +
  scale_y_continuous(breaks = 1:18, labels = 1:18) +
  theme(panel.grid.major.x = element_blank(),
              panel.grid.minor.x = element_blank()) +
  scale_shape_manual(name = "", labels = c("Median","Mean", "Mode"),
                                values = c(0, 2, 19)) +
  scale_size_manual(name = "", labels = c("Median", "Mean", "Mode"), 
                               values = c(4, 3, 1)) +
  guides(shape = "legend")
```

### Norm Results Graph

``` {r normgraph}
  
###GRAPH FOR NORM MODEL###

#transpose data to tidy form for use in ggplot
SimResultsnorm.2018$simNumber <- sprintf("sim%d", seq(1:Nsim.norm.2018))
SimResultsnorm.2018.tidy <- gather(SimResultsnorm.2018,"Team", "Position", -simNumber)

#create ordered factor to order graph
SimResultsnorm.2018.tidy$Team <- factor(SimResultsnorm.2018.tidy$Team, levels = Finalsimtablenorm.2018$Team)

#create graph 
ggplot(SimResultsnorm.2018.tidy, aes(x = Team, y = Position)) + 
  geom_violin(aes(fill = Team, col = Team), show.legend = FALSE,
              trim = TRUE, kernel = "gaussian", adjust = 3) + 
  stat_summary(fun.y=median, geom="point",color="black", 
                     aes(shape = "Median", size="Median")) +
  stat_summary(fun.y=mean, geom ="point", color = "black",
                     aes(shape = "Mean", size = "Mean")) +
  stat_summary(fun.y=Mode, geom ="point", color = "black",
                     aes(shape = "Mode", size = "Mode")) +
  coord_flip(ylim = c(1,18)) +
  scale_y_continuous(breaks = 1:18, labels = 1:18) +
  theme(panel.grid.major.x = element_blank(),
              panel.grid.minor.x = element_blank()) +
  scale_shape_manual(name = "", labels = c("Median","Mean", "Mode"),
                                values = c(0, 2, 19)) +
  scale_size_manual(name = "", labels = c("Median", "Mean", "Mode"), 
                               values = c(4, 3, 1)) +
  guides(shape = "legend")

```

### Rankings Model Graph

```{r tanhgraph}
###Graph for Rankings Model###

#transpose data to tidy form for use in ggplot
SimResultstanh.2018 <- SimResultsList.2018[[20]] %>% as.data.frame()
SimResultstanh.2018$simNumber <- sprintf("sim%d", seq(1:Nsim.tanh.2018))
SimResultstanh.2018.tidy <- gather(SimResultstanh.2018,"Team", "Position", -simNumber)

#create ordered factor to order graph
SimResultstanh.2018.tidy$Team <- factor(SimResultstanh.2018.tidy$Team, levels = Finalsimlist.2018[[w]]$Team)

ggplot(SimResultsnorm.2018.tidy, aes(x = Team, y = Position)) + 
  geom_violin(aes(fill = Team, col = Team), show.legend = FALSE,
              trim = TRUE, kernel = "gaussian", adjust = 3) + 
  stat_summary(fun.y=median, geom="point",color="black", 
                     aes(shape = "Median", size="Median")) +
  stat_summary(fun.y=mean, geom ="point", color = "black",
                     aes(shape = "Mean", size = "Mean")) +
  stat_summary(fun.y=Mode, geom ="point", color = "black",
                     aes(shape = "Mode", size = "Mode")) +
  coord_flip(ylim = c(1,18)) +
  scale_y_continuous(breaks = 1:18, labels = 1:18) +
  theme(panel.grid.major.x = element_blank(),
              panel.grid.minor.x = element_blank()) +
  scale_shape_manual(name = "", labels = c("Median","Mean", "Mode"),
                                values = c(0, 2, 19)) +
  scale_size_manual(name = "", labels = c("Median", "Mean", "Mode"), 
                               values = c(4, 3, 1)) +
  guides(shape = "legend")
```

### Comparison Graphs

``` {r comparisongraphs}
## create comparison graphs ##

#combine two graphs##
Finalsimtablenorm.2018$Model <- "norm"
Finalsimtablepois.2018$Model <- "pois"
Finalsimlist.2018[[20]]$Model <- "tanh"

comparisonSimTable.2018 <- rbind(Finalsimtablenorm.2018, Finalsimtablepois.2018, Finalsimlist.2018[[20]])
comparisonSimTable.2018$Model <- factor(comparisonSimTable.2018$Model)

# create tidy data to compare a lot in one go
comparisonSimTable.2018.tidy <- gather(comparisonSimTable.2018, "Type", "Percent", 6:10)

#create ordered factor to order graph
comparisonSimTable.2018.tidy$Team <- factor(comparisonSimTable.2018.tidy$Team, levels = Finalsimtablepois.2018$Team)

#create ordered factor of 'Type' to order facet
comparisonSimTable.2018.tidy$Type <- factor(comparisonSimTable.2018.tidy$Type, 
                                       levels = c("percentWon", "percentTop4", "percentTop8",
                                                  "percentBottom4", "percentLast"))

#draw graphs
ggplot(comparisonSimTable.2018, aes(x = Team, y = percentWon, fill = Model)) + 
  geom_bar(stat = "identity", position = 'dodge') +
  theme(axis.text.x=element_text(angle = 45, hjust = 1))

ggplot(comparisonSimTable.2018, aes(x = Team, y = percentTop8, fill = Model)) +
  geom_bar(stat = "identity", position = 'dodge') +
  theme(axis.text.x=element_text(angle = 45, hjust = 1))

ggplot(comparisonSimTable.2018.tidy, aes(x = Team, y = Percent, fill = Model)) + 
  geom_bar(stat = "identity", position = 'dodge') + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 6)) +
  facet_wrap(~Type, nrow = 3)

```
      
## Conclusion 

Upon analysis of this data, it seems to me that the 'normal' model is the best of the bunch.  It allows for the variability that we know, from a cursory knowledge of the sport, exists from year to year, while still preserving relevant information from last year. 

In Part II of this series we apply this data to the upcoming season --- to predict the all but certain future that that the Cats will once again be premier! (I wish)

See you then.
