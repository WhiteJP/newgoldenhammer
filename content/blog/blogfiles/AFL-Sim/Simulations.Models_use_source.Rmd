---
title: "AFL Simulation Stimulation"
subtitle: "Part 1: Modelling the 2018 Season"
author: "Joshua White"
date: "26 March 2019"
output:
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

source("AFLsim2018.R")
```

Recently I was, as I am (unfortunately) rather wont to do, aimlessly watching YouTube videos when I came across these [two](https://www.youtube.com/watch?v=Vv9wpQIGZDw) [videos](https://www.youtube.com/watch?v=Zs2M7gWSbTg) by Numberphile. Here, the authors discuss their attempts at modelling and simulating seasons of the English Premier League. 

As an avid [AFL](https://en.wikipedia.org/wiki/Australian_Football_League) fan, and with the upcoming season fast approaching (edit, now underway, [carn](https://www.urbandictionary.com/define.php?term=carn) [the cats!](https://www.afl.com.au/news/2019-03-22/match-report-collingwood-v-geelong)), why not try to apply these methods to the AFL season.  This post will be the first of a three part series. In part 1, I will discuss the models and compare their results to the actual data from the 2018 season. Part 2 will apply the models to the 2019 season. And part 3 will go through the details and show the code.  

## The Models

###Poisson Distributed Team Scores
Okay, lets discuss the model. Basically, each team's score in every game is modeled as a random drawing from a Poisson distribution --- i.e. $P(k) = e^{-\lambda} \frac{\lambda^k}{k!}$, where k is the number of points scored. A Poisson distribution is great for simple modelling because it only requires estimating one parameter, $\lambda$. On the other hand, it isn't exactly suited to the AFL context because the model assumes that all events (in this case, scoring a point) occur independently. In AFL, a goal in AFL is worth 6 points, violating this assumption.  

In any case, let's press on with that in mind. We calculate $\lambda$ for each game, based on the points scored for and against, at home and away, for each team in 2018:

$\lambda_{home team} =$ League Average points scored at home $\times$ home team attack strength at home $\times$ awayteam attack strength away

$\lambda_{away team} =$ League Average points scored away $\times$ away team attack strength away $\times$ hometeam defense strength at home

As you can see, the model takes into account the general home advantage as well as each particular team's attack and defensive strengths at home and away. The strength parameters refereed to above are simply the averages for that particular team, relative to the league-wide average. 

###Normally Distributed team scores
This model is a simple extension to the Poisson-based model above, and should be superior to it because: 

1. There is no assumption of independence, and 
2. It has an extra parameter to capture differences among teams in the variance of their defense and attack strength. 

Here, it is assumed that team scores are normally distributed around $\lambda$ as defined above, with a standard deviation calculated as the pooled standard deviation of that teams 'for' scores standard deviation, and the opponents 'against' scores standard deviation:
$$Points_{teamA} \sim \mathcal{N}(\mu=\lambda,\,\sigma = \sigma_p)\,$$ 
where: <br>
$$\sigma_{p}=\frac{\sigma_{teamAfor} + \sigma_{teamBagainst}}{2}$$

###Rankings model: tanh
This model simply relies on the ordinal data of last years final Ladder: Carlton, the wooden spoon winners (i.e. the last placeholders, out of 18 teams) get a ranking of 1, and Richmond, who finished on top of the table get a ranking of 18. Then, with this information, the probability of team A winning is modelled as follows:

$$P(A\>wins)=\frac{tanh(\frac{a-b}{w} + d)+1}{2}$$
where,<br>
A = home team <br>
a = home team ranking <br> 
b = away team ranking <br>
w = weighting parameter (the higher the number the less weight placed on the rankings) <br>
d = draw paramater, to allow for possibility of draws, calculated as so that $tanh(d) = -draw\>rate$

The draw rate, $\frac{7}{990} \approx 0.007$, was based on the likelihood of drawing a game over the past 5 years.

The probability of B winning is calculated in the same manner as above, except with $b-a$. 

Finally, a draw is the remaining probability: $P(Draw)= 1 - P(A\>wins) - P(B\>wins)$. This means that the draw rate will be greater for greater rankings differentials, and greater for greater weight parameters. 

## The Data

```{r data}
pointstidy
fixture2019
```


##Results

###Poisson model
```{r poisresults}
Finalsimtablepois.2018

```

###Normal model
```{r normresults}
Finalsimtablenorm.2018

```

###Tanh model

```{r tanhresults}
Finalsimlist.2018[[1]] %>% tibble()
Finalsimlist.2018[[20]] %>% tibble()
Finalsimlist.2018[[50]] %>% tibble()
Finalsimlist.2018[[100]] %>% tibble()
Finalsimlist.2018[[500]] %>% tibble()
Finalsimlist.2018[[1000]] %>% tibble()

```

OKay, so we don't have to keep doing simulations with all these different weight parameters, let's see which ones do the best job of simulating the data. We are going to have to pay it by ear a little based on intuition, however, because we have used the same data to test the model, and we want to avoid overfitting. So we will pick the best model based on (1) it's spearman coefficient with last year's ladder and (2) its ability to have varation (the same team doesn't simply win each year) 

```{r tanhcomparisons}
##Create order comparisons table## 
#should change this to use lapply or try to put loop insid dataframe fucntion  
for (w in c(1, 20, 50, 100, 500, 1000)){
  name <- paste0("Ordertanh.", w)
  assign(name,  match(finalTable2018$Team, Finalsimlist.2018[[w]]$Team))
  
}

ordercomparison.2018.tanh <- data.frame(Team = factor(finalTable2018$Team),
                                        Order2018 = 1:18,
                                        Ordertanh.1,
                                        Ordertanh.20,
                                        Ordertanh.50,
                                        Ordertanh.100, 
                                        Ordertanh.500,
                                        Ordertanh.1000)
ordercomparison.2018.tanh
cormatrix.weights <- cor(ordercomparison.2018.tanh[,2:ncol(ordercomparison.2018.tanh)], use = "all.obs", method = "spearman")

# lets find a nice way to graphically look at that
ggcorrplot(cormatrix.weights, lab = TRUE, show.legend = TRUE)
```

So on this basis, we will choose a weight parameter value of 20 as, on inspection of simulation results, it allows for enough randomness in the system to allow teams in the top 8 some likelihood of winning the cup, but denies the chance for the really teams for winning. Let's be honest, carlton's chance of winning the premiership is less than 1 in a million. 

##Comparing the three models.

```{r comparing the models}

#create table with different orders. 
ordercomparison.2018 <- data.frame(Team = factor(finalTable2018$Team),
                              Order2018 = 1:18,
                              OrderNorm.2018 = match(finalTable2018$Team, Finalsimtablenorm.2018$Team),
                              OrderPois.2018 = match(finalTable2018$Team, Finalsimtablepois.2018$Team),
                              OrderTanh.2018 = match(finalTable2018$Team, Finalsimlist.2018[[20]]$Team)) 

ordercomparison.2018 %>% tibble()

#Let's now get the spearman's correlation coefficients and show that in a correlatin matrix
cormatrix.2018 <- cor(ordercomparison.2018[,2:5], use = "all.obs", method = "spearman")
ggcorrplot(cormatrix.2018, lab = TRUE)
```

```{r graphs}
#graphing and comparing the two models. 

#create mode function
Mode <- function(x) {
  ux <- unique(x)
  ux[which.max(tabulate(match(x, ux)))]
}
###GRAPH FOR POISSON MODEL###

#transpose data to tidy form for use in ggplot
SimResultspois.2018$simNumber <- sprintf("sim%d", seq(1:Nsim.pois.2018))
SimResultspois.2018.tidy <- gather(SimResultspois.2018,"Team", "Position", -simNumber)


#create ordered factor to order graph
SimResultspois.2018.tidy$Team <- factor(SimResultspois.2018.tidy$Team, levels = Finalsimtablepois.2018$Team)

#create graph 
ggplot(SimResultspois.2018.tidy, aes(x = Team, y = Position)) + 
  geom_violin(aes(fill = Team, col = Team), trim = TRUE, kernel = "gaussian", adjust = 3) + 
  stat_summary(fun.y=median, geom="point", size=4, color="black", shape = 0) +
  stat_summary(fun.y=mean, geom="point", size=3, color="black", shape = 2) +
  stat_summary(fun.y=Mode, geom="point", size=1, color="black", shape = 19) +
  coord_flip(ylim = c(1,18)) +
  scale_y_continuous(breaks = 1:18, labels = 1:18) +
  theme(legend.position = "none", panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) 
  
###GRAPH FOR NORM MODEL###

#transpose data to tidy form for use in ggplot
SimResultsnorm.2018$simNumber <- sprintf("sim%d", seq(1:Nsim.norm.2018))
SimResultsnorm.2018.tidy <- gather(SimResultsnorm.2018,"Team", "Position", -simNumber)

#create ordered factor to order graph
SimResultsnorm.2018.tidy$Team <- factor(SimResultsnorm.2018.tidy$Team, levels = Finalsimtablenorm.2018$Team)

#create graph 
ggplot(SimResultsnorm.2018.tidy, aes(x = Team, y = Position)) + 
  geom_violin(aes(fill = Team, col = Team), trim = TRUE, kernel = "gaussian", adjust = 3) + 
  stat_summary(fun.y=median, geom="point", size=4, color="black", shape = 0) +
  stat_summary(fun.y=mean, geom="point", size=3, color="black", shape = 2) +
  stat_summary(fun.y=Mode, geom="point", size=1, color="black", shape = 19) +
  coord_flip(ylim = c(1,18)) +
  scale_y_continuous(breaks = 1:18, labels = 1:18) +
  theme(legend.position = "none", panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank()) 


## create comparison graphs ##

#combine two graphs##
Finalsimtablenorm.2018$Model <- "norm"
Finalsimtablepois.2018$Model <- "pois"
Finalsimlist.2018[[20]]$Model <- "tanh"

comparisonSimTable.2018 <- rbind(Finalsimtablenorm.2018, Finalsimtablepois.2018, Finalsimlist.2018[[20]])
comparisonSimTable.2018$Model <- factor(comparisonSimTable.2018$Model)

# create tidy data to compare a lot in one go
comparisonSimTable.2018.tidy <- gather(comparisonSimTable.2018, "Type", "Percent", 6:10)
comparisonSimTable.2018.tidy 

#create ordered factor to order graph
comparisonSimTable.2018.tidy$Team <- factor(comparisonSimTable.2018.tidy$Team, levels = Finalsimtablepois.2018$Team)

#create ordered factor of 'Type' to order facet
comparisonSimTable.2018.tidy$Type <- factor(comparisonSimTable.2018.tidy$Type, 
                                       levels = c("percentWon", "percentTop4", "percentTop8",
                                                  "percentBottom4", "percentLast"))

#draw graphs
ggplot(comparisonSimTable.2018, aes(x = Team, y = percentWon, fill = Model)) + 
  geom_bar(stat = "identity", position = 'dodge')
ggplot(comparisonSimTable.2018, aes(x = Team, y = percentTop8, fill = Model)) +
  geom_bar(stat = "identity", position = 'dodge')

ggplot(comparisonSimTable.2018.tidy, aes(x = Team, y = Percent, fill = Model)) + 
  geom_bar(stat = "identity", position = 'dodge') + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 6)) +
  facet_wrap(~Type, nrow = 1)

## WHich of these two models best captures last years fnal table.
# Personally, I think the normal distribution model is by far the best of these two, It alligns with the variable nature of afl football and gives more beleiveable results.
# It also allows for teams which footy intution says have a chance of winning the cup to win the cup. # but lets see which dataset best comports with last years data. 
# lets import last years finaltable data.  
#also note that comparing with last year isnt the  best method, because we have a different fixture (unlike football in the UK everyteam does not play eachother twice )

```